
class Llama2Template(Template):



    def _encode(

        self,

        tokenizer: "PreTrainedTokenizer",

        system: str,

        history: List[Tuple[str, str]]

    ) -> List[Tuple[List[int], List[int]]]:

        r"""

        Encodes formatted inputs to pairs of token ids.

        Turn 0: bos + prefix + query    resp + eos

        Turn t: bos + query             resp + eos

        """

        bos_ids, eos_ids = self._get_special_ids(tokenizer)

        encoded_pairs = []

        for turn_idx, (query, resp) in enumerate(history):

            if turn_idx == 0: # llama2 template has no sep_ids

                query = self.prefix[0].replace("{{system}}", system) + query

            query_ids = self._convert_inputs_to_ids(tokenizer, context=self.prompt, query=query)

            resp_ids = self._convert_inputs_to_ids(tokenizer, context=[resp])

            encoded_pairs.append((bos_ids + query_ids, resp_ids + eos_ids))

        return encoded_pairs





templates: Dict[str, Template] = {}





def register_template(

    name: str,

    prefix: List[Union[str, Dict[str, str]]],

    prompt: List[Union[str, Dict[str, str]]],

    system: str,

    sep: List[Union[str, Dict[str, str]]],

    stop_words: Optional[List[str]] = [],

    use_history: Optional[bool] = True,

    efficient_eos: Optional[bool] = False

) -> None:

    template_class = Llama2Template if "llama2" in name else Template

    templates[name] = template_class(

        prefix=prefix,

        prompt=prompt,

        system=system,

        sep=sep,

        stop_words=stop_words,

        use_history=use_history,

        efficient_eos=efficient_eos

    )





def get_template_and_fix_tokenizer(

    name: str,

    tokenizer: "PreTrainedTokenizer"

) -> Template:

    if tokenizer.eos_token_id is None:

        tokenizer.eos_token = "<|endoftext|>"

        logger.info("Add eos token: {}".format(tokenizer.eos_token))



    if tokenizer.pad_token_id is None:

        tokenizer.pad_token = tokenizer.eos_token

        logger.info("Add pad token: {}".format(tokenizer.pad_token))



    if name is None:

        return None



    template = templates.get(name, None)

    assert template is not None, "Template {} does not exist.".format(name)

    tokenizer.add_special_tokens(

        dict(additional_special_tokens=template.stop_words),

        replace_additional_special_tokens=False

    )

    return template





r"""

Supports language model inference without histories.

"""

register_template(

    name="vanilla",

    prefix=[],

    prompt=[

        "{{query}}"

    ],

    system="",

    sep=[],

    use_history=False

)





r"""

Default template.

"""

register_template(

    name="default",

    prefix=[

        "{{system}}"

    ],

    prompt=[

        "Human: {{query}}\nAssistant: "

    ],

    system=(

        "A chat between a curious user and an artificial intelligence assistant. "

        "The assistant gives helpful, detailed, and polite answers to the user's questions."

    ),

    sep=[

        "\n"

    ]

)





r"""

Supports: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf

          https://huggingface.co/meta-llama/Llama-2-13b-chat-hf

          https://huggingface.co/meta-llama/Llama-2-70b-chat-hf

"""

register_template(

    name="llama2",

    prefix=[

        "<<SYS>>\n{{system}}\n<</SYS>>\n\n"

    ],

    prompt=[

        "[INST] {{query}} [/INST] "

    ],

    system=(

        "You are a helpful, respectful and honest assistant. "

        "Always answer as helpfully as possible, while being safe.  "

        "Your answers should not include any harmful, unethical, "

        "racist, sexist, toxic, dangerous, or illegal content. "

        "Please ensure that your responses are socially unbiased and positive in nature.\n\n"

        "If a question does not make any sense, or is not factually coherent, "

        "explain why instead of answering something not correct. "

        "If you don't know the answer to a question, please don't share false information."

    ),

    sep=[]

)





r"""

Supports: https://github.com/ymcui/Chinese-LLaMA-Alpaca-2

          https://huggingface.co/ziqingyang/chinese-alpaca-2-7b

"""

register_template(

    name="llama2_zh",

    prefix=[

        "<<SYS>>\n{{system}}\n<</SYS>>\n\n"

    ],

    prompt=[

        "[INST] {{query}} [/INST] "

    ],

    system="You are a helpful assistant. 你是一个乐于助人的助手。",

    sep=[]

)





r"""

Supports: https://huggingface.co/tatsu-lab/alpaca-7b-wdiff

          https://github.com/ymcui/Chinese-LLaMA-Alpaca

"""

register_template(

    name="alpaca",

    prefix=[

        "{{system}}"

    ],

    prompt=[

        "### Instruction:\n{{query}}\n\n### Response:\n"

    ],

    system=(

        "Below is an instruction that describes a task. "

        "Write a response that appropriately completes the request."

    ),

    sep=[

        "\n\n"

    ]

)





r"""

Supports: https://huggingface.co/lmsys/vicuna-7b-delta-v1.1

          https://huggingface.co/lmsys/vicuna-13b-delta-v1.1

"""

register_template(

    name="vicuna",

    prefix=[

        "{{system}}"

    ],

    prompt=[

        "USER: {{query}} ASSISTANT: "

    ],

    system=(

        "A chat between a curious user and an artificial intelligence assistant. "

        "The assistant gives helpful, detailed, and polite answers to the user's questions."

    ),

    sep=[]

)





r"""

Supports: https://huggingface.co/BelleGroup/BELLE-LLaMA-EXT-13B

"""

register_template(

    name="belle",

    prefix=[

        "{{system}}"

    ],

    prompt=[

        "Human: {{query}}\n\nBelle: "

    ],

    system="",

    sep=[

        "\n\n"

    ]

)





r"""

Supports: https://github.com/CVI-SZU/Linly

"""

register_template(

    name="linly",

    prefix=[

        "{{system}}"

    ],

    prompt=[

        "User: {{query}}\nBot: "

    ],

    system="",

    sep=[

        "\n"

    ]

)





r"""

Supports: https://github.com/Neutralzz/BiLLa

"""

register_template(

    name="billa",

    prefix=[

        "{{system}}"

    ],

    prompt=[

        "Human: {{query}}\nAssistant: "

    ],

    system="",

    sep=[

        "\n"

    ]

)





r"""

Supports: https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1

"""

register_template(

    name="ziya",

    prefix=[

        "{{system}}"

    ],

    prompt=[

        {"token": "<human>"},

        ":{{query}}\n",

        {"token": "<bot>"},

        ":"

    ],

    system="",

    sep=[

        "\n"

    ]

)





r"""

Supports: https://huggingface.co/BAAI/AquilaChat-7B

"""

register_template(

    name="aquila",

    prefix=[

        "{{system}}"

    ],

    prompt=[

        "Human: {{query}}###Assistant: "

    ],

    system=(

        "A chat between a curious human and an artificial intelligence assistant. "

        "The assistant gives helpful, detailed, and polite answers to the human's questions."

    ),

    sep=[

        "###"

    ],

    stop_words=[

        "</s>"

    ],

    efficient_eos=True

)





r"""

Supports: https://huggingface.co/internlm/internlm-chat-7b

"""

register_template(

    name="intern",

    prefix=[

        "{{system}}"

    ],

    prompt=[

        "<|User|>:{{query}}",

        {"token": "<eoh>"},

        "\n<|Bot|>:"

    ],

    system="",

    sep=[

        {"token": "<eoa>"},

        "\n"

    ],

    stop_words=[

        "<eoa>"

    ],

    efficient_eos=True

)





r"""

Supports: https://huggingface.co/baichuan-inc/Baichuan-13B-Chat

"""

register_template(

    name="baichuan",

    prefix=[

        "{{system}}"

    ],

    prompt=[

        {"token": "<reserved_102>"}, # user token

        "{{query}}",

        {"token": "<reserved_103>"}  # assistant token

    ],

    system="",

    sep=[],

    efficient_eos=True

)





r"""

Supports: https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat

          https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat

"""

register_template(

    name="baichuan2",

    prefix=[

        "{{system}}"

    ],

    prompt=[

        {"token": "<reserved_106>"}, # user token

        "{{query}}",

        {"token": "<reserved_107>"}  # assistant token

    ],

    system="",

    sep=[],

    efficient_eos=True

)





r"""

Supports: https://huggingface.co/HuggingFaceH4/starchat-alpha

          https://huggingface.co/HuggingFaceH4/starchat-beta

"""

register_template(

    name="starchat",

    prefix=[

        {"token": "<|system|>"},

        "\n{{system}}",

    ],

    prompt=[

        {"token": "<|user|>"},

        "\n{{query}}",

        {"token": "<|end|>"},

        "\n",

        {"token": "<|assistant|>"}

    ],

    system="",

    sep=[

        {"token": "<|end|>"},

        "\n"

    ],

    stop_words=[

        "<|end|>"

    ],

    efficient_eos=True

)





r"""

Supports: https://huggingface.co/Qwen/Qwen-7B-Chat

"""

register_template(

    name="chatml",

    prefix=[

        {"token": "<|im_start|>"},

        "system\n{{system}}"

    ],

    prompt=[

        {"token": "<|im_start|>"},

        "user\n{{query}}",

        {"token": "<|im_end|>"},

        "\n",

        {"token": "<|im_start|>"},

        "assistant\n"

    ],

    system="You are a helpful assistant.",

    sep=[

        {"token": "<|im_end|>"},

        "\n"

    ],

    stop_words=[

        "<|im_end|>"

    ],

    efficient_eos=True

)





r"""

Supports: https://huggingface.co/THUDM/chatglm2-6b

"""

register_template(

    name="chatglm2",

    prefix=[

        {"token": "[gMASK]"},

        {"token": "sop"},

        "{{system}}"

    ],

    prompt=[

        "[Round {{idx}}]\n\n问：{{query}}\n\n答："

    ],

    system="",

    sep=[

        "\n\n"

    ],

    efficient_eos=True

)





r"""

Supports: https://huggingface.co/xverse/XVERSE-13B-Chat

"""

register_template(

    name="xverse",

    prefix=[

        "{{system}}"

    ],

    prompt=[

        "Human: {{query}}\n\nAssistant: "

    ],

    system="",

    sep=[]

)







    def preprocess_supervised_dataset(examples: Dict[str, List[Any]]) -> Dict[str, Any]:

        # build inputs with format `<bos> X Y <eos>` and labels with format `<ignore> ... <ignore> Y <eos>`

        # for multiturn examples, we only mask the prompt part in each prompt-response pair.

        model_inputs = {"input_ids": [], "attention_mask": [], "labels": []}



        for query, response, history, system in construct_example(examples):

            input_ids, labels = [], []



            for turn_idx, (source_ids, target_ids) in enumerate(template.encode_multiturn(

                tokenizer, query, response, history, system

            )):

                total_len = len(source_ids) + len(target_ids)

                max_source_len = int(data_args.cutoff_len * (len(source_ids) / total_len))

                max_target_len = int(data_args.cutoff_len * (len(target_ids) / total_len))



                if len(source_ids) > max_source_len:

                    source_ids = source_ids[:max_source_len]

                if len(target_ids) > max_target_len:

                    target_ids = target_ids[:max_target_len]



                if data_args.train_on_prompt:

                    source_mask = source_ids

                elif turn_idx != 0 and template.efficient_eos:

                    source_mask = [tokenizer.eos_token_id] + [IGNORE_INDEX] * (len(source_ids) - 1)

                else:

                    source_mask = [IGNORE_INDEX] * len(source_ids)



                input_ids += source_ids + target_ids

                labels += source_mask + target_ids



            if template.efficient_eos:

                input_ids += [tokenizer.eos_token_id]

                labels += [tokenizer.eos_token_id]



            if len(input_ids) > data_args.cutoff_len:

                input_ids = input_ids[:data_args.cutoff_len]

                labels = labels[:data_args.cutoff_len]



            model_inputs["input_ids"].append(input_ids)

            model_inputs["attention_mask"].append([1] * len(input_ids))

            model_inputs["labels"].append(labels)



        return model_inputs
